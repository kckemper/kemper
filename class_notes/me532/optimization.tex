\subsection{Unconstrained Optimization Problems}
\begin{itemize}
\item Functions of a single variable. Let the function $f: \Re \rightarrow \Re$. First order necessary conditions for a minimum at $x = x^*$
$$f'(x) = \frac{d}{dx}f(x^*) = 0.$$
Second order sufficiency conditions
$$f''(x) = \frac{d^2}{dx^2}f(x^*) > 0.$$
If $f:[a,b]\rightarrow\Re$, then the minimum could occur at $a$ or $b$.


\item Functions of multiple variables. Let the function $f: \Re^n \rightarrow \Re$, example $f(x_1, x_2, x_3):\Re^3 \rightarrow \Re$. First order necessary conditions for a minimum at $x = x^*$
$$\grad f(x^*) = 0.$$ Second order sufficiency conditions  $$\grad^2 f(x^*) >0, $$ that is, the matrix is positive definite. To be precise,
$$\grad f(x^*) = \left[ \frac{\pd f}{\pd x_1}(x^*), \frac{\pd f}{\pd x_2}(x^*), ..., \frac{\pd f}{\pd x_n}(x^*)\right]^T = 0$$
$$\grad^2 f(x^*) = \left[\begin{array}{cccc} \frac{\pd^2 f}{\pd x_1^2}(x^*)& \frac{\pd f^2}{\pd x_1 \pd x_2}(x^*)& ... & \frac{\pd f^2}{\pd x_1\pd x_n}(x^*)\\[1em]
\frac{\pd^2 f}{\pd x_2\pd x_1}(x^*)& \frac{\pd f^2}{\pd x_2^2}(x^*)& ... & \frac{\pd f^2}{\pd x_2\pd x_n}(x^*)\\[2em]
\frac{\pd^2 f}{\pd x_n\pd x_1}(x^*)& \frac{\pd f^2}{\pd x_n \pd x_2}(x^*)& ... & \frac{\pd f^2}{\pd x_n^2}(x^*)
\end{array}\right] {\rm positive\, definite}.$$


\end{itemize}
Example problems
\newpage

\subsection{Optimization with Static Equality Constraints}
Standard problem takes the form
$$\min f(x), f:\Re^n \rightarrow \Re$$
\hspace{.15in}{\it subject to}
$$ g(x) = C, g:\Re^n\rightarrow\Re^m$$
Example:  $$\ds \min f(x) = \frac 12 \left(\frac{x_1^2}{a^2}+\frac{x_2^2}{b^2}\right)$$
\hspace{1.5in}{\it subject to} $$x_1 + mx_2 = C$$

\begin{itemize}
\item Solution approach 1: eliminate constraint through substitution and treat as unconstrained problem
\newpage
\item Solution approach 2: look at geometry of problem.  A minimizer can only occur where constrain set is tangent to $f$. At these tangent points, the gradient of $f$ is parallel to the gradient of $g$. Hence, there exists a constant $\lambda$ so that $\grad f(x^*) = \lambda \grad g(x^*)$ (if $g$ maps to $\Re^m$, then $\lambda$ is a vector of size $m$). Define the Lagrangian to be $$H(x) = f(x) + \lambda^Tg(x), \quad {\rm for} \lambda \in \Re^m.$$  The gradient of $H$ is zero at $x^*$ means that the gradient of $f$ is parallel to the gradient of $g$ at $x^*$. 
\end{itemize}
Example continued...
\newpage
